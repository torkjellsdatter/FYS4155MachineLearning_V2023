{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ae416e",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek43.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises weeks 43 and 44  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfcd25c",
   "metadata": {},
   "source": [
    "# Exercises weeks 43 and 44 \n",
    "**October 9-13, 2023**\n",
    "\n",
    "Date: **Deadline is Sunday November 5 at midnight**\n",
    "\n",
    "You can hand in the exercises from week 43 and week 44 as one exercise and get a total score of two additional points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7dbe4",
   "metadata": {},
   "source": [
    "# Overarching aims of the exercises weeks 43 and 44\n",
    "\n",
    "The aim of the exercises this week and next week is to get started with writing a neural network code\n",
    "of relevance for project 2. \n",
    "\n",
    "During week 41 we discussed three different types of gates, the\n",
    "so-called XOR, the OR and the AND gates.  In order to develop a code\n",
    "for neural networks, it can be useful to set up a simpler system with\n",
    "only two inputs and one output. This can make it easier to debug and\n",
    "study the feed forward pass and the back propagation part. In the\n",
    "exercise this and next week, we propose to study this system with just\n",
    "one hidden layer and two hidden nodes. There is only one output node\n",
    "and we can choose to use either a simple regression case (fitting a\n",
    "line) or just a binary classification case with the cross-entropy as\n",
    "cost function.\n",
    "\n",
    "Their inputs and outputs can be\n",
    "summarized using the following tables, first for the OR gate with\n",
    "inputs $x_1$ and $x_2$ and outputs $y$:\n",
    "\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">$x_1$</th> <th align=\"center\">$x_2$</th> <th align=\"center\">$y$</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   0        </td> <td align=\"center\">   0        </td> <td align=\"center\">   0      </td> </tr>\n",
    "<tr><td align=\"center\">   0        </td> <td align=\"center\">   1        </td> <td align=\"center\">   1      </td> </tr>\n",
    "<tr><td align=\"center\">   1        </td> <td align=\"center\">   0        </td> <td align=\"center\">   1      </td> </tr>\n",
    "<tr><td align=\"center\">   1        </td> <td align=\"center\">   1        </td> <td align=\"center\">   1      </td> </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108a242",
   "metadata": {},
   "source": [
    "## The AND and XOR Gates\n",
    "\n",
    "The AND gate is defined as\n",
    "\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">$x_1$</th> <th align=\"center\">$x_2$</th> <th align=\"center\">$y$</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   0        </td> <td align=\"center\">   0        </td> <td align=\"center\">   0      </td> </tr>\n",
    "<tr><td align=\"center\">   0        </td> <td align=\"center\">   1        </td> <td align=\"center\">   0      </td> </tr>\n",
    "<tr><td align=\"center\">   1        </td> <td align=\"center\">   0        </td> <td align=\"center\">   0      </td> </tr>\n",
    "<tr><td align=\"center\">   1        </td> <td align=\"center\">   1        </td> <td align=\"center\">   1      </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "And finally we have the XOR gate\n",
    "\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">$x_1$</th> <th align=\"center\">$x_2$</th> <th align=\"center\">$y$</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   0        </td> <td align=\"center\">   0        </td> <td align=\"center\">   0      </td> </tr>\n",
    "<tr><td align=\"center\">   0        </td> <td align=\"center\">   1        </td> <td align=\"center\">   1      </td> </tr>\n",
    "<tr><td align=\"center\">   1        </td> <td align=\"center\">   0        </td> <td align=\"center\">   1      </td> </tr>\n",
    "<tr><td align=\"center\">   1        </td> <td align=\"center\">   1        </td> <td align=\"center\">   0      </td> </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6993a7",
   "metadata": {},
   "source": [
    "## Representing the Data Sets\n",
    "\n",
    "Our design matrix is defined by the input values $x_1$ and $x_2$. Since we have four possible outputs, our design matrix reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd0efa",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix} 0 & 0 \\\\\n",
    "                       0 & 1 \\\\\n",
    "\t\t       1 & 0 \\\\\n",
    "\t\t       1 & 1 \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba74e1",
   "metadata": {},
   "source": [
    "while the vector of outputs is $\\boldsymbol{y}^T=[0,1,1,0]$ for the XOR gate, $\\boldsymbol{y}^T=[0,0,0,1]$ for the AND gate and $\\boldsymbol{y}^T=[0,1,1,1]$ for the OR gate.\n",
    "\n",
    "Your tasks here are\n",
    "\n",
    "1. Set up the design matrix with the inputs as discussed above and a vector containing the output, the so-called targets. Note that the design matrix is the same for all gates. You need just to define different outputs.\n",
    "\n",
    "2. Construct a neural network with only one hidden layer and two hidden nodes using the Sigmoid function as activation function.\n",
    "\n",
    "3. Set up the output layer with only one output node and use again the Sigmoid function as activation function for the output.\n",
    "\n",
    "4. Initialize the weights and biases and perform a feed forward pass and compare the outputs with the targets.\n",
    "\n",
    "5. Set up the cost function (cross entropy for classification of binary cases).\n",
    "\n",
    "6. Calculate the gradients needed for the back propagation part.\n",
    "\n",
    "7. Use the gradients to train the network in the back propagation part. Think of using automatic differentiation.\n",
    "\n",
    "8. Train the network and study your results and compare with results obtained either with **scikit-learn** or **TensorFlow**.\n",
    "\n",
    "Everything you develop here can be used directly into the code for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4b4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc912815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS(X, y): \n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b309acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ffc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Defining the data \n",
    "# Design matrix\n",
    "X = np.array([ [0, 0], [0, 1], [1, 0],[1, 1]],dtype=np.float64)\n",
    "\n",
    "# targets\n",
    "# The XOR gate\n",
    "yXOR = np.array( [ 0, 1 ,1, 0])\n",
    "# The OR gate\n",
    "yOR = np.array( [ 0, 1 ,1, 1])\n",
    "# The AND gate\n",
    "yAND = np.array( [ 0, 0 ,0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28eef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of theta for the XOR gate:[0.33333333 0.33333333]\n",
      "The linear regression prediction  for the XOR gate:[0.         0.33333333 0.33333333 0.66666667]\n",
      "The values of theta for the OR gate:[0.66666667 0.66666667]\n",
      "The linear regression prediction  for the OR gate:[0.         0.66666667 0.66666667 1.33333333]\n",
      "The values of theta for the AND gate:[0.33333333 0.33333333]\n",
      "The linear regression prediction  for the AND gate:[0.         0.33333333 0.33333333 0.66666667]\n"
     ]
    }
   ],
   "source": [
    "# linear regression baseline = testing\n",
    "thetaXOR = OLS(X, yXOR)\n",
    "print(f\"The values of theta for the XOR gate:{thetaXOR}\")\n",
    "print(f\"The linear regression prediction  for the XOR gate:{X @ thetaXOR}\")\n",
    "# output should be: 0, 1, 1, 0 (i.e. OLS does a bad job)\n",
    "\n",
    "thetaOR = OLS(X, yOR)\n",
    "print(f\"The values of theta for the OR gate:{thetaOR}\")\n",
    "print(f\"The linear regression prediction  for the OR gate:{X @ thetaOR}\")\n",
    "# output should be: 0, 1, 1, 1 (i.e. OLS does a bad job)\n",
    "\n",
    "thetaAND = OLS(X, yAND)\n",
    "print(f\"The values of theta for the AND gate:{thetaAND}\")\n",
    "print(f\"The linear regression prediction  for the AND gate:{X @ thetaAND}\")\n",
    "# output should be: 0, 0, 0, 1 (i.e. OLS does a bad job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583cb686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer:  0\n",
      "Probability:  0.839397225553582 0.8325766934302149\n"
     ]
    }
   ],
   "source": [
    "# understanding the math:\n",
    "x1 = X[0, 0]\n",
    "x2 = X[0, 1]\n",
    "\n",
    "b1, b2 = np.zeros(2) + 0.01 # bias (1, 2)\n",
    "weights = np.random.randn(2, 2) # weights (2, 2)\n",
    "w11 = weights[0, 0]\n",
    "w12 = weights[0, 1]\n",
    "w21 = weights[1, 0]\n",
    "w22 = weights[1, 1]\n",
    "\n",
    "y1 = (w11*x1) + (w12*x2) + b1 # output of node 1\n",
    "y2 = (w12*x2) + (w22*x2) + b2# output of node 2\n",
    "\n",
    "# activation of the hidden layer\n",
    "h1 = sigmoid(y1)\n",
    "h2 = sigmoid(y2)\n",
    "\n",
    "# compute the output: should be two dimentional (prob of being 0, prob of being 1)\n",
    "# i.e. there is two output nodes\n",
    "ow11, ow12, ow21, ow22 = 0.7, 0.8, 0.7, 0.8\n",
    "ob1, ob2 = 0.9, 0.8\n",
    "z1 = (ow11*h1) + (ow12*h2) + ob1 # output of outout node 1\n",
    "z2 = (ow12*h2) + (ow22*h2) + ob2# output of output node 2\n",
    "\n",
    "output1 = sigmoid(z1)\n",
    "output2 = sigmoid(z2)\n",
    "\n",
    "print(\"Correct answer: \", yXOR[0])\n",
    "print(\"Probability: \", output1, output2) # slightly higher probability of being 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2d129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "Correct answer:  0\n",
      "Probability:  [0.83939723 0.82545468]\n"
     ]
    }
   ],
   "source": [
    "# From writing it out to matrix multiplication\n",
    "x = X[0]\n",
    "n_hidden_nodes = 2\n",
    "n_features = x.shape[0] # reformulate when passing more data\n",
    "n_categories = 2\n",
    "\n",
    "#hidden_bias = np.zeros(n_hidden_nodes) + 0.01\n",
    "#hidden_weights = np.random.randn(n_features, n_hidden_nodes)\n",
    "hidden_bias = b1, b2\n",
    "hidden_weights = weights\n",
    "\n",
    "y = (hidden_weights @ x) + hidden_bias\n",
    "\n",
    "# activate the hidden layer\n",
    "h = sigmoid(y)\n",
    "print(y.shape, h.shape)\n",
    "\n",
    "#output_weights = np.random.randn(n_hidden_nodes, n_categories) # (2, 1)\n",
    "#output_bias = np.zeros(n_categories) + 0.01 # 1\n",
    "output_weights = np.array([[ow11, ow12], [ow21, ow22]])\n",
    "output_bias = np.array([ob1, ob2])\n",
    "z = (output_weights @ h) + output_bias\n",
    "\n",
    "output = sigmoid(z)\n",
    "print(z.shape, output.shape)\n",
    "\n",
    "print(\"Correct answer: \", yXOR[0])\n",
    "print(\"Probability: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4bb7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer:  0\n",
      "Probability:  [0.49967347]\n"
     ]
    }
   ],
   "source": [
    "# Make even more abstract with method\n",
    "def feed_forward(X):\n",
    "    \n",
    "    y_h = (X @ hidden_weights) + hidden_bias # be aware of order of multiplication\n",
    "    \n",
    "    #activate the hidden layer\n",
    "    a_h = sigmoid(y_h)\n",
    "    \n",
    "    # output layer\n",
    "    z_o = (a_h @ output_weights) + output_bias\n",
    "    \n",
    "    # activate output layer \n",
    "    probabilities = sigmoid(z_o)\n",
    "    \n",
    "    return(a_h, probabilities)\n",
    "\n",
    "a_h, prob = feed_forward(x)\n",
    "print(\"Correct answer: \", yXOR[0])\n",
    "print(\"Probability: \", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "93b0dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities_to_binary(predictions, threshold=0.5):\n",
    "    return [1 if p >= threshold else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "886d8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b8eb431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer:  [0 1 1 1]\n",
      "Probability:  [[0.69795047]\n",
      " [0.72991706]\n",
      " [0.59711011]\n",
      " [0.64663108]]\n",
      "Prediction:  [1, 1, 1, 1]\n",
      "Quantify the success of the model:  0.75\n"
     ]
    }
   ],
   "source": [
    "# For the whole data set, random number of features\n",
    "# Defining the neural network\n",
    "n_inputs, n_features = X.shape\n",
    "n_hidden_neurons = 2\n",
    "n_categories = 1\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01\n",
    "\n",
    "a_h, probabilities = feed_forward(X)\n",
    "print(\"Correct answer: \", yOR)\n",
    "print(\"Probability: \", probabilities)\n",
    "y_pred = probabilities_to_binary(probabilities)\n",
    "print(\"Prediction: \", y_pred)\n",
    "## most common methods of evaluating classification: accuracy, confusion, precision, recall, AUC, log los, MSE can also be usedm but less commin\n",
    "print(\"Quantify the success of the model: \", accuracy_score(yOR, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878dd913",
   "metadata": {},
   "source": [
    "### 5. Set up the cost function (cross entropy for classification of binary cases).\n",
    "Cross entropy / log loss: importan cost function usd to optimize classification models. \n",
    "Softmax: often put in the last layer - activation function that sales numbers into probabilities.\n",
    "Sigmoid is suitable when you have a single output neuron, whereas softmax is used for multi-class problems. The sigmoid function produces a singøe output between 0 and 1 which is an estimate of the positive class.The softmax function maps a vector of K real numbers to a normalized probability distribtion consisting of K probabilities, i.e. the output sums to 1. Both sigmoid and softmax are commonly used in conjuction with the cross-entropy loss function. \n",
    "\n",
    "Cross entropy: prediction is compared to actual class, and a score is calculated that penalizes the probability based on how far it is from the actual expectation value. The penalty is logaritmic yielding a large score for large differences close to 1 and a small score for a small differenecs tendin to 0. \n",
    "\n",
    "Cross entropy: the same as the maximum likelihood function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "973cc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred): # from 14.1.1. \n",
    "    # y_pred is the output of the activation function a_i = y_i = exp(z_i)/(1 + exp(z_i))\n",
    "    # use of np.mean rather than np.sum like the definiton says: taking the mean gets you the average loss per data point\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) # make sure that y_pred is not 0 or 1 for wich logarithm is not defined\n",
    "    return  -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b4338",
   "metadata": {},
   "source": [
    "### 6. Calculate the gradients needed for the back propagation part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4d922",
   "metadata": {},
   "source": [
    "The loss funcion is $L = - \\sum^n_{i=1}(t_i \\log a_i^L + (1 - t_i) \\log (1- a_i^L)$, where $t_1$ is the targer atd $a_i$ is the predicted probability from the sigmoid function. And \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial a_i^L}  = \\frac{a_i^L-t_i}{a_i^L(1- a_i^L)}$$\n",
    "\n",
    "But what we would like is the derivative of the cost function with respect to the input to the neuron, not the predicted probability. Thus \n",
    "$$ \\frac{\\partial L}{\\partial z_j^L} = \\frac{\\partial L}{\\partial a_i^L} \\frac{d \\sigma}{d z_j^L} $$\n",
    "\n",
    "And since $\\sigma$ is the sigmoid function it has the derivative $ \\frac{d \\sigma}{d z_j^L} = \\sigma(z)(1- \\sigma(z))$, Thus \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_j^L} = \\frac{a_i^L-t_i}{a_i^L(1- a_i^L)} \\cdot  \\sigma(z)(1- \\sigma(z))$$\n",
    "\n",
    "But since $\\sigma(z) = a_i^L \\rightarrow \\frac{\\partial L}{\\partial z_j^L} = a_i^L-t_i $ \n",
    "\n",
    "In other words: predicted - true values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3be02777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, Y):\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "    Y = Y.reshape(-1, 1) \n",
    "    \n",
    "    # 1) error in the output layer\n",
    "    # the gradient of the loss with respect to the output z_o binary cross entropy cost function (see above)\n",
    "    # is the activation of the outout layer minus the targets \n",
    "    error_output = probabilities - Y\n",
    "    # 2) error in the hidden layer\n",
    "    # in order to propagate the error from the output layer to the hidden layer, we need to consider two things; \n",
    "    # 1. the weight of teh connections between the layers (output_weights.T)\n",
    "    # 2. The gradient of the loss with respect to the output\n",
    "    # i.e. np.matmul(error_output, output_weights.T) gives the error at the output of the hidden layer\n",
    "    # in order to adjust the weights connecting the hidden layer to the INPUT layer we need to know \n",
    "    # how much this error is due to the changes in the hidden layers input z_h\n",
    "    # therefore we need the derivative of the hidden layers activation function, i.e. a_h * (1 - a_h)\n",
    "    error_hidden = np.matmul(error_output, output_weights.T) * a_h * (1 - a_h)\n",
    "    \n",
    "    # 3) gradients for the output layer\n",
    "    # Multiply activations in the hidden layer with the error in the output ->\n",
    "    # How does the error change as I tweak each weight of the output layer?? \n",
    "    output_weights_gradient = np.matmul(a_h.T, error_output)\n",
    "    # sums errors over all samples, represents how much the bias term contribute to the overall error. \n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    # these gradients can be used to update the weithus and bias in the NN \n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755bc5e",
   "metadata": {},
   "source": [
    "### 7. Use the gradients to train the network in the back propagation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4ad33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we obtain a prediction by taking the class with the highest likelihood\n",
    "def predict(X):\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "    return probabilities_to_binary(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7a8d960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer:  [0 1 1 0]\n",
      "Predicted:  [1, 1, 0, 0]\n",
      "New accuracy on training data: 0.5\n"
     ]
    }
   ],
   "source": [
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01\n",
    "\n",
    "eta = 0.01 # should be tuned. \n",
    "lmbd = 0.01 # regularization: penalize large weights thus prevent overfitting\n",
    "for i in range(1000):\n",
    "    # calculate gradients\n",
    "    dWo, dBo, dWh, dBh = backpropagation(X, yXOR)\n",
    "    \n",
    "    # regularization term gradients\n",
    "    dWo += lmbd * output_weights\n",
    "    dWh += lmbd * hidden_weights\n",
    "    \n",
    "    # update weights and biases\n",
    "    output_weights -= eta * dWo\n",
    "    output_bias -= eta * dBo\n",
    "    hidden_weights -= eta * dWh\n",
    "    hidden_bias -= eta * dBh\n",
    "\n",
    "# in order to test (metric of choice), run one more time through the FFNN with optimized weights and biases\n",
    "print(\"Correct answer: \", yXOR)\n",
    "print(\"Predicted: \", predict(X))\n",
    "print(\"New accuracy on training data: \" + str(accuracy_score(predict(X), yXOR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96bd5a",
   "metadata": {},
   "source": [
    "6. Calculate the gradients needed for the back propagation part.\n",
    "\n",
    "7. Use the gradients to train the network in the back propagation part. Think of using automatic differentiation.\n",
    "\n",
    "8. Train the network and study your results and compare with results obtained either with **scikit-learn** or **TensorFlow**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
